{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cfdb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neusum\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9142f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "log = logging.getLogger(__name__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLINE_PROCESS_DATA = True\n",
    "STRIPPING_MODE = 'none' #'none', 'normal', 'magnitude', 'topk'\n",
    "MAX_SENT_LENGTH = 80\n",
    "MAX_DOC_LEN = 500\n",
    "NORM_LAMBDA = 20\n",
    "THRESHOLD = 0.0001\n",
    "SAVE_PATH = ''\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "DROP_TOO_SHORT = 50\n",
    "DROP_TOO_LONG = 500\n",
    "DEC_INIT = 'simple' # simple, att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715018c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_online(\n",
    "    train_src: str, \n",
    "    src_vocab: str, \n",
    "    train_tgt, \n",
    "    tgt_vocab, \n",
    "    train_oracle, \n",
    "    train_src_rouge, \n",
    "    src_section: str, \n",
    "    drop_too_short: int = 10, \n",
    "    drop_too_long: int = 500, \n",
    "    bert_annotation: str = ''\n",
    "):\n",
    "    dicts = {}\n",
    "    dicts['src'] = initVocabulary('source', [train_src], src_vocab, 1000000)\n",
    "    dicts['tgt'] = None\n",
    "\n",
    "    logger.info('Preparing training ...')\n",
    "    train = {}\n",
    "    train['src'], train['src_raw'], train['tgt'], \\\n",
    "        train['oracle'], train['src_rouge'], \\\n",
    "        train['src_section'], train['src_section_raw'], train['bert_annotation'] = makeData(train_src, train_tgt,\n",
    "                                                       train_oracle, train_src_rouge, src_section,\n",
    "                                                       dicts['src'], dicts['tgt'],\n",
    "                                                       drop_too_short, drop_too_long, bert_annotation)\n",
    "\n",
    "    dataset = { 'dicts': dicts, 'train': train, }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0dac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModel(model: nn.Module, \n",
    "              summarizer, \n",
    "              evalData, \n",
    "              output_len: int = 1, \n",
    "              prefix: str = 'dev', \n",
    "              postfix: str = '', \n",
    "              stripping_mode: str = STRIPPING_MODE, \n",
    "              specifyEpoch: int = -1):\n",
    "    \"\"\"\n",
    "    Output length is used for debug purpose, meant to output more sentence at once.\n",
    "    Make sure the beam_size is greater or equal to n_best_size.\n",
    "    (The code is warped in the if output_len > 1)\n",
    "\n",
    "\n",
    "    TODO: involve log-linear model in decoding?!\n",
    "    \"\"\"\n",
    "    global evalModelCount\n",
    "\n",
    "    if specifyEpoch <= 0:\n",
    "        specifyEpoch = evalModelCount\n",
    "\n",
    "    predict, gold, predict_sents, attnScore, topkPred, gold_sents = getLabel(\n",
    "        summarizer, \n",
    "        evalData, \n",
    "        output_len, \n",
    "        threshold=0.0001, \n",
    "        stripping_mode=stripping_mode, \n",
    "        isEval=True\n",
    "    )\n",
    "\n",
    "    scores_total = compute_selection_acc(gold, predict)\n",
    "    scores_hit1 = compute_selection_acc(gold, predict, hit1mode=True)\n",
    "    scores_metrics = compute_metric(gold, predict)\n",
    "    scores_bleu_raw = compute_bleu_raw(\n",
    "        gold_sents, predict_sents, gold, predict)\n",
    "\n",
    "    if postfix:\n",
    "        postfix = '.' + postfix\n",
    "\n",
    "    with open(os.path.join(SAVE_PATH, '{1}{2}.out.{0}'.format(specifyEpoch, prefix, postfix)), 'w', encoding='utf-8') as of:\n",
    "        for p, sent in zip(predict, predict_sents):\n",
    "            of.write('{0}\\t{1}'.format(sent, p) + '\\n')\n",
    "\n",
    "    if output_len > 1:\n",
    "\n",
    "        with open(os.path.join(SAVE_PATH, '{1}{2}.{3}_n_out.{0}'.format(specifyEpoch, prefix, postfix, output_len)), 'w', encoding='utf-8') as of:\n",
    "            for p, sent, score, topk_idx in zip(predict, predict_sents, attnScore, topkPred):\n",
    "                of.write('{0}\\t{1}\\t{2}\\t{3}\\n'.format(\n",
    "                             sent, \n",
    "                             p, \n",
    "                             tuple(s.tolist() for s in score), \n",
    "                             tuple(ti.tolist() for ti in topk_idx))\n",
    "                        )\n",
    "                # note that, the topk_idx is not necessary be the same as idx,\n",
    "                # since beam search finds the highest score of a single route,\n",
    "                # so it is possible that it didn't select the \"max attention\" score index on a step\n",
    "\n",
    "    return [scores_total, scores_hit1, scores_metrics, scores_bleu_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dev_data(summarizer, \n",
    "                  src_file: str, \n",
    "                  oracl_file: str, \n",
    "                  src_section_file: str, \n",
    "                  drop_too_short: int = 10, \n",
    "                  drop_too_long: int = 500, \n",
    "                  test_bert_annotation: str = '', \n",
    "                  postfix: str = '', \n",
    "                  qtype: str = ''):\n",
    "    \"\"\"\n",
    "    Load dev/test set data. (similar with makeData in onlinePreprocess.py)\n",
    "    \"\"\"\n",
    "\n",
    "    def addPair(f1, f2, f3, f4=None):\n",
    "        if not f4:\n",
    "            for x, x2, y1 in zip(f1, f2, f3):\n",
    "                yield (x, x2, y1, None)\n",
    "            yield (None, None, None, None)\n",
    "        else:\n",
    "            for x, x2, y1, y2 in zip(f1, f2, f3, f4):\n",
    "                yield (x, x2, y1, y2)\n",
    "            yield (None, None, None, None)\n",
    "\n",
    "    if postfix:\n",
    "        # assert if postfix is given, then it is running on train data\n",
    "        keywords = loglinear.Config.Keyword[qtype]\n",
    "        use_good = True\n",
    "    else:\n",
    "        # normal case\n",
    "        keywords = []\n",
    "        use_good = False\n",
    "\n",
    "    # here tgt is sentence index\n",
    "    seq_length = MAX_SENT_LENGTH\n",
    "    dataset, raw = [], []\n",
    "    src_raw, tgt_raw = [], []\n",
    "    src_section_raw, src_section_batch = [], []\n",
    "    src_batch, tgt_batch = [], []\n",
    "    oracle_batch = []\n",
    "    srcF = open(src_file, encoding='utf-8')\n",
    "    srcSectionF = open(src_section_file, encoding='utf-8')\n",
    "    # tgtF = open(tgt_file, encoding='utf-8')\n",
    "    oracleF = open(oracl_file, encoding='utf-8')\n",
    "\n",
    "    if test_bert_annotation:\n",
    "        bertF = open(test_bert_annotation, encoding='utf-8')\n",
    "        bert_annotation_batch = []\n",
    "    else:\n",
    "        bertF = None\n",
    "\n",
    "    for sline, secline, oline, bline in addPair(srcF, srcSectionF, oracleF, bertF):\n",
    "        if (sline is not None) and (oline is not None):\n",
    "            if sline == \"\" or oline == \"\":\n",
    "                continue\n",
    "            sline = sline.strip()\n",
    "            secline = secline.strip()\n",
    "            oline = oline.strip()\n",
    "            if test_bert_annotation:\n",
    "                bline = bline.strip()\n",
    "            srcSents = sline.split('##SENT##')\n",
    "            srcSectionSents = secline.split('##SENT##')\n",
    "\n",
    "            if len(srcSents) < drop_too_short or len(srcSents) > drop_too_long:\n",
    "                logger.info('Drop data too short or too long')\n",
    "                continue\n",
    "\n",
    "            # this will transfer string of tuple to tuple\n",
    "            oracle_combination = make_tuple(oline.split('\\t')[0])\n",
    "            oracle_combination = [x for x in oracle_combination]  # no sentinel\n",
    "            if test_bert_annotation:\n",
    "                bert_annotation_combination = make_tuple(bline.split('\\t')[0])\n",
    "                bert_annotation_combination = [\n",
    "                    x for x in bert_annotation_combination]  # no sentinel\n",
    "            srcWords = [x.split(' ')[:seq_length] for x in srcSents]\n",
    "            srcSectionWords = [x.split(' ')[:seq_length]\n",
    "                               for x in srcSectionSents]\n",
    "            # tgtWords = ' '.join(tgtSents)\n",
    "            src_raw.append(srcSents)\n",
    "            src_batch.append(srcWords)\n",
    "            src_section_raw.append(srcSectionSents)\n",
    "            src_section_batch.append(srcSectionWords)\n",
    "            # tgt_raw.append(tgtWords)\n",
    "            oracle_batch.append(torch.LongTensor(oracle_combination))\n",
    "            if test_bert_annotation:\n",
    "                bert_annotation_batch.append(\n",
    "                    torch.LongTensor(bert_annotation_combination))\n",
    "\n",
    "            if len(src_batch) < BATCH_SIZE:\n",
    "                continue\n",
    "        else:\n",
    "            # at the end of file, check last batch\n",
    "            if len(src_batch) == 0:\n",
    "                break\n",
    "        if test_bert_annotation:\n",
    "            data = summarizer.buildData(\n",
    "                src_batch, src_raw, None, oracle_batch, None, src_section_batch, src_section_raw, bert_annotation=bert_annotation_batch, good_patterns=keywords, use_good=use_good)\n",
    "        else:\n",
    "            data = summarizer.buildData(\n",
    "                src_batch, src_raw, None, oracle_batch, None, src_section_batch, src_section_raw, good_patterns=keywords, use_good=use_good)\n",
    "        dataset.append(data)\n",
    "        src_batch, tgt_batch = [], []\n",
    "        src_raw, tgt_raw = [], []\n",
    "        src_section_raw, src_section_batch = [], []\n",
    "        oracle_batch = []\n",
    "        if test_bert_annotation:\n",
    "            bert_annotation_batch = []\n",
    "\n",
    "    srcF.close()\n",
    "    # tgtF.close()\n",
    "    oracleF.close()\n",
    "    if test_bert_annotation:\n",
    "        bertF.close()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47791c21",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eec007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neusum.Summarizer.Summarizer"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ONLINE_PROCESS_DATA:\n",
    "    logger.info('Online Preprocessing data (to get vocabulary dictionary).')\n",
    "    onlinePreprocess.seq_length = MAX_SENT_LENGTH\n",
    "    onlinePreprocess.max_doc_len = MAX_DOC_LEN\n",
    "    onlinePreprocess.shuffle = 1\n",
    "    onlinePreprocess.norm_lambda = NORM_LAMBDA\n",
    "    dataset = prepare_data_online(\n",
    "        opt.train_src, \n",
    "        opt.src_vocab, \n",
    "        opt.train_tgt, \n",
    "        opt.tgt_vocab, \n",
    "        opt.train_oracle,\n",
    "        opt.train_src_rouge, \n",
    "        opt.train_src_section, \n",
    "        opt.drop_too_short, \n",
    "        opt.drop_too_long\n",
    "    )\n",
    "else:\n",
    "    logger.info('Use preprocessed data stored in checkpoint.')\n",
    "    dataset = {} # this is used for the summarizer (only need the 'dict' part)\n",
    "\n",
    "logger.info('Loading checkpoint...')\n",
    "if opt.specific_epoch > 0:\n",
    "    model_selected = os.path.join(SAVE_PATH, 'model_epoch_%s.pt' % opt.specific_epoch)\n",
    "    logger.info('Loading from the specific epoch checkpoint \"%s\"' %\n",
    "                model_selected)\n",
    "else:\n",
    "    # Find the latest model to load\n",
    "    model_path = glob(os.path.join(SAVE_PATH, '*.pt'))\n",
    "    if not model_path:\n",
    "        raise ValueError(\"Can't find model %s\" %\n",
    "                         os.path.join(SAVE_PATH, '*.pt'))\n",
    "\n",
    "    # make sure not load the log linear model\n",
    "    model_selected = None\n",
    "    for candidate in reversed(sorted(model_path, key=os.path.getmtime)):\n",
    "        if 'log_linear' not in candidate:\n",
    "            model_selected = candidate\n",
    "            break\n",
    "    assert model_selected is not None\n",
    "    logger.info('Loading from the latest model \"%s\"' % model_selected)\n",
    "\n",
    "    checkpoint = torch.load(model_selected, map_location=device)\n",
    "\n",
    "logger.info('\\tprevious training epochs: %d' % checkpoint['epoch'])\n",
    "\n",
    "if not ONLINE_PROCESS_DATA:\n",
    "    dataset['dicts'] = checkpoint['dicts']\n",
    "dicts = checkpoint['dicts']\n",
    "\n",
    "logger.info(' * vocabulary size. source = %d' %\n",
    "            (dicts['src'].size()))\n",
    "if ONLINE_PROCESS_DATA:\n",
    "    logger.info(' * number of training sentences. %d' % len(dataset['train']['src']))\n",
    "logger.info(' * maximum batch size. %d' % BATCH_SIZE)\n",
    "\n",
    "logger.info('Building model...')\n",
    "\n",
    "sent_encoder = neusum.Models.Encoder(opt, dicts['src'])\n",
    "doc_encoder = neusum.Models.DocumentEncoder(opt)\n",
    "pointer = neusum.Models.Pointer(opt)\n",
    "if DEC_INIT == \"simple\":\n",
    "    decIniter = neusum.Models.DecInit(opt)\n",
    "elif DEC_INIT == \"att\":\n",
    "    decIniter = neusum.Models.DecInitAtt(opt)\n",
    "else:\n",
    "    raise ValueError('Unknown decoder init method: {0}'.format(DEC_INIT))\n",
    "\n",
    "model = neusum.Models.NMTModel(sent_encoder, doc_encoder, pointer, decIniter).to(device)\n",
    "\n",
    "# load model\n",
    "logger.info('Loading trained model...')\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "summarizer = neusum.Summarizer(opt, model, dataset)\n",
    "\n",
    "\n",
    "testData = load_dev_data(summarizer, opt.dev_input_src, opt.dev_ref,\n",
    "                         opt.dev_input_src_section, test_bert_annotation=opt.test_bert_annotation)\n",
    "model.eval()\n",
    "scores = evalModel(model, summarizer, testData,\n",
    "                   opt.output_len, 'test', opt.set_postfix, opt.stripping_mode, checkpoint['epoch'])\n",
    "logger.info('Using checkpoint: %s' % model_selected)\n",
    "logger.info('Key hyperparmeters:')\n",
    "logger.info('\\tMax Decode Steps: %d' % opt.max_decode_step)\n",
    "logger.info('\\tKeep Data: [%d, %d)' % (opt.drop_too_short, opt.drop_too_long))\n",
    "logger.info('\\tTrained epoch: %s' % checkpoint['epoch'])\n",
    "logger.info('Evaluate score: (accuracy) total / hit@1, {precision, recall, f1 score} (sentence-level)')\n",
    "logger.info(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ddeae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
